
This research explores neural synchrony in syntactic processing, particularly focusing on how the brain processes minimal phrases through different modalities (audio and visual) and rhythmic patterns (isochronous and anisochronous). The study is conducted using Electroencephalography (EEG) data, combined with machine learning techniques to analyze brain responses.

Research Goals
The study aims to:

Investigate how the brain reacts to different sensory modalities (visual vs. auditory) and rhythmicity (isochronous vs. anisochronous).
Use machine learning models to classify EEG signals and reveal patterns in neural activity related to syntactic processing.
Provide insights into potential real-world applications such as brain-computer interfaces and diagnostic tools for sensory processing disorders.
Methodology
Data Collection
Participants: 40 native English speakers.
EEG System: 64-channel NeuroScan QuikCap system.
Stimuli: Combinations of auditory and visual word pairs with unifiable or non-unifiable syntactic structures. Isochronous and anisochronous conditions were applied in auditory experiments.
Data Preprocessing
Filtering: Applied bandpass filters (0.1 Hz to 100 Hz) and a Butterworth filter to remove noise.
Artifact Removal: Independent Component Analysis (ICA) and manual artifact rejection were used to clean the data.
Analysis
Key Features: Time-series analysis, Power Spectrum Density (PSD), and Inter-Trial Phase Coherence (ITPC) were used to observe neural activity.
Machine Learning: Various models, including SVM, CNN, DNN, Random Forest, KNN, and Gradient Boosting, were applied to classify EEG data and predict brain responses.
Results
Machine Learning: CNN outperformed other models in terms of accuracy, revealing significant patterns in how the brain processes different modalities and rhythmicity.
Neural Synchrony: Differences were observed in neural synchrony based on both the sensory modality (audio vs. visual) and the rhythmicity (isochronous vs. anisochronous) of stimuli.
